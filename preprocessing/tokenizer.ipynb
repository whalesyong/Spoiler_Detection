{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac0a794a-ecb2-487f-a707-863ca1fb877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "import os \n",
    "\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3725256b-974a-47b6-b631-f280917920c3",
   "metadata": {},
   "source": [
    "### Pre-processing of Book Corpus Refined \n",
    "The dataset is from Kaggle, `nishantsingh96/refined-bookcorpus-dataset`.\\\n",
    "We preprocess the data, even though the author claims its been already preprocessed. We'll use the polars library for faster processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52f931dc-d03f-49aa-9a28-8dbfcc67c7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # remove unnecessary symbols (keep punctuation)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,!?;:'\\\"()\\- ]+\", \" \", text)\n",
    "    \n",
    "    # normalize white space\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    \n",
    "    # text has allegedly been lowered for us. But, we do this so that we can apply it on the downstream dataset\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "def filter_text(text: str, min_length: int = 10, max_length: int = 10000) -> bool:\n",
    "    \"\"\"Filter text based on length criteria.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "    \n",
    "    text_length = len(text.strip())\n",
    "    return min_length <= text_length <= max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58d62b15-c6f6-40da-aee5-5efcb29c8659",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "data_path = os.path.expanduser(\"~/BookCorpus/BookCorpus3.csv\")\n",
    "# Example: reading CSV\n",
    "df = pl.read_csv(data_path)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7207675b-e1d6-4217-bee1-2558f7a1730b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (11_043_119, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>0</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;she began getting up first thi…</td></tr><tr><td>&quot;caitrin hid the fact that she …</td></tr><tr><td>&quot;she considered strapping down …</td></tr><tr><td>&quot;as hard as it had been for cai…</td></tr><tr><td>&quot;as her treks took her farther …</td></tr><tr><td>&hellip;</td></tr><tr><td>&quot;you can&#x27;t fight him she shoute…</td></tr><tr><td>&quot;a ha war eagle exclaimed. he l…</td></tr><tr><td>&quot;it was the next afternoon befo…</td></tr><tr><td>&quot;the story of zorana has been t…</td></tr><tr><td>&quot;thank you as well to austin ni…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (11_043_119, 1)\n",
       "┌─────────────────────────────────┐\n",
       "│ 0                               │\n",
       "│ ---                             │\n",
       "│ str                             │\n",
       "╞═════════════════════════════════╡\n",
       "│ she began getting up first thi… │\n",
       "│ caitrin hid the fact that she … │\n",
       "│ she considered strapping down … │\n",
       "│ as hard as it had been for cai… │\n",
       "│ as her treks took her farther … │\n",
       "│ …                               │\n",
       "│ you can't fight him she shoute… │\n",
       "│ a ha war eagle exclaimed. he l… │\n",
       "│ it was the next afternoon befo… │\n",
       "│ the story of zorana has been t… │\n",
       "│ thank you as well to austin ni… │\n",
       "└─────────────────────────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5762af1a-2536-4603-9551-ca0579cb1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = (\n",
    "    df.with_columns([\n",
    "        pl.col('0').map_elements(clean_text, return_dtype=pl.Utf8).alias(\"cleaned_text\")\n",
    "    ]).filter(\n",
    "            # Filter out empty strings and texts outside length bounds\n",
    "            (pl.col(\"cleaned_text\").str.len_chars() >= 10) &\n",
    "            (pl.col(\"cleaned_text\").str.len_chars() <= 10000) &\n",
    "            (pl.col(\"cleaned_text\") != \"\")\n",
    "        )\n",
    ")\n",
    "# save it immediately \n",
    "df_processed.write_csv('bookcorpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55674628-c8d9-4f39-806f-2c714bfc50f3",
   "metadata": {},
   "source": [
    "### Training a tokenizer \n",
    "We'll do a BPE tokenizer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78fce458-fd57-42ef-913d-bb4864e31c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=30000, \n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "files = [\"bookcorpus.txt\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96ea9f6c-3d6d-46e8-a62c-811bfd2129f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train and save the tokenizer when ready! \n",
    "tokenizer.train(files, trainer)\n",
    "tokenizer.save(\"bpe_tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d07026e-22f1-41b5-acf9-d7a9578d5d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Example usage:\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "hf_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"bpe_tokenizer.json\")\n",
    "hf_tokenizer.pad_token = \"[PAD]\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba55db1d-ec2a-47bc-bfa8-5225f00dc9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w/weiyong/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "hf_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"bpe_tokenizer.json\")\n",
    "hf_tokenizer.pad_token = \"[PAD]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47ec92e7-f811-41d7-af94-58d0f4479ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded IDs: [52, 772, 1995, 6113, 12040, 259, 52, 8817, 1701, 10]\n",
      "Decoded Text: the quick brown fox jumps over the lazy dog .\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"the quick brown fox jumps over the lazy dog.\"\n",
    "encoded = hf_tokenizer.encode(sample_text)\n",
    "print(\"Encoded IDs:\", encoded)\n",
    "decoded = hf_tokenizer.decode(encoded)\n",
    "print(\"Decoded Text:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65a76d6-3139-4d76-82de-b33063d4fc8d",
   "metadata": {},
   "source": [
    "### Pre-processing IMDb dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b38cf3e-69ad-42a0-93f3-f988b8c06ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_path = os.path.expanduser(\"~/datasets/imdb_spoiler/IMDB_reviews.json\")\n",
    "ds_df = pl.read_ndjson(imdb_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f22392-4093-4afc-91c0-dcaf6b8b704d",
   "metadata": {},
   "source": [
    "### Splitting into Train and Test \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca095fbb-2a97-4e86-8cdd-4711373b0796",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df = ds_df.sample(fraction=1.0, shuffle=True, seed=42)  # reproducible split\n",
    "\n",
    "# Train-test split (e.g., 80/20)\n",
    "split_idx = int(0.8 * len(ds_df))\n",
    "train_df = ds_df[:split_idx]\n",
    "test_df = ds_df[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e7fb3cb-3061-422c-ad88-b7e9113cff7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (459_130, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>review_date</th><th>movie_id</th><th>user_id</th><th>is_spoiler</th><th>review_text</th><th>rating</th><th>review_summary</th></tr><tr><td>str</td><td>str</td><td>str</td><td>bool</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;31 July 2001&quot;</td><td>&quot;tt0126029&quot;</td><td>&quot;ur0341961&quot;</td><td>false</td><td>&quot;this movie was so good.&nbsp;&nbsp;&nbsp;I me…</td><td>&quot;10&quot;</td><td>&quot;And I went without little peop…</td></tr><tr><td>&quot;19 December 2003&quot;</td><td>&quot;tt0167260&quot;</td><td>&quot;ur2513127&quot;</td><td>false</td><td>&quot;I am saddened to learn that Se…</td><td>&quot;10&quot;</td><td>&quot;Fantastic!!! No other words&quot;</td></tr><tr><td>&quot;15 June 2007&quot;</td><td>&quot;tt0449088&quot;</td><td>&quot;ur15706936&quot;</td><td>false</td><td>&quot;Why are they making more Pirat…</td><td>&quot;1&quot;</td><td>&quot;What the?&quot;</td></tr><tr><td>&quot;11 April 2002&quot;</td><td>&quot;tt0137523&quot;</td><td>&quot;ur1685153&quot;</td><td>false</td><td>&quot;David Fincher, Edward Norton, …</td><td>&quot;10&quot;</td><td>&quot;What a ride!&quot;</td></tr><tr><td>&quot;10 July 2015&quot;</td><td>&quot;tt1823672&quot;</td><td>&quot;ur48448272&quot;</td><td>false</td><td>&quot;Chappie 2015 is an extremely a…</td><td>&quot;10&quot;</td><td>&quot;simply amazing!&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;8 August 2006&quot;</td><td>&quot;tt0454848&quot;</td><td>&quot;ur9034662&quot;</td><td>true</td><td>&quot;Well, he doesn&#x27;t have me foole…</td><td>&quot;2&quot;</td><td>&quot;don&#x27;t bother&quot;</td></tr><tr><td>&quot;24 November 2010&quot;</td><td>&quot;tt0464154&quot;</td><td>&quot;ur19669890&quot;</td><td>true</td><td>&quot;Fish. Tits. Fish. Tits. Fish. …</td><td>&quot;3&quot;</td><td>&quot;A Portion Of Fish And Tits, Pl…</td></tr><tr><td>&quot;16 June 2011&quot;</td><td>&quot;tt1605783&quot;</td><td>&quot;ur2444068&quot;</td><td>true</td><td>&quot;Gil (Owen Wilson) is a self-de…</td><td>&quot;8&quot;</td><td>&quot;A dream ride in a classic Fren…</td></tr><tr><td>&quot;23 November 2012&quot;</td><td>&quot;tt0454876&quot;</td><td>&quot;ur6983748&quot;</td><td>false</td><td>&quot;Astounding movie experience!! …</td><td>&quot;10&quot;</td><td>&quot;Movie of the Year 2012&quot;</td></tr><tr><td>&quot;12 August 2006&quot;</td><td>&quot;tt0415306&quot;</td><td>&quot;ur4839121&quot;</td><td>false</td><td>&quot;I was extremely disappointed i…</td><td>&quot;3&quot;</td><td>&quot;A mediocre movie and a terribl…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (459_130, 7)\n",
       "┌─────────────────┬───────────┬────────────┬────────────┬────────────────┬────────┬────────────────┐\n",
       "│ review_date     ┆ movie_id  ┆ user_id    ┆ is_spoiler ┆ review_text    ┆ rating ┆ review_summary │\n",
       "│ ---             ┆ ---       ┆ ---        ┆ ---        ┆ ---            ┆ ---    ┆ ---            │\n",
       "│ str             ┆ str       ┆ str        ┆ bool       ┆ str            ┆ str    ┆ str            │\n",
       "╞═════════════════╪═══════════╪════════════╪════════════╪════════════════╪════════╪════════════════╡\n",
       "│ 31 July 2001    ┆ tt0126029 ┆ ur0341961  ┆ false      ┆ this movie was ┆ 10     ┆ And I went     │\n",
       "│                 ┆           ┆            ┆            ┆ so good.   I   ┆        ┆ without little │\n",
       "│                 ┆           ┆            ┆            ┆ me…            ┆        ┆ peop…          │\n",
       "│ 19 December     ┆ tt0167260 ┆ ur2513127  ┆ false      ┆ I am saddened  ┆ 10     ┆ Fantastic!!!   │\n",
       "│ 2003            ┆           ┆            ┆            ┆ to learn that  ┆        ┆ No other words │\n",
       "│                 ┆           ┆            ┆            ┆ Se…            ┆        ┆                │\n",
       "│ 15 June 2007    ┆ tt0449088 ┆ ur15706936 ┆ false      ┆ Why are they   ┆ 1      ┆ What the?      │\n",
       "│                 ┆           ┆            ┆            ┆ making more    ┆        ┆                │\n",
       "│                 ┆           ┆            ┆            ┆ Pirat…         ┆        ┆                │\n",
       "│ 11 April 2002   ┆ tt0137523 ┆ ur1685153  ┆ false      ┆ David Fincher, ┆ 10     ┆ What a ride!   │\n",
       "│                 ┆           ┆            ┆            ┆ Edward Norton, ┆        ┆                │\n",
       "│                 ┆           ┆            ┆            ┆ …              ┆        ┆                │\n",
       "│ 10 July 2015    ┆ tt1823672 ┆ ur48448272 ┆ false      ┆ Chappie 2015   ┆ 10     ┆ simply         │\n",
       "│                 ┆           ┆            ┆            ┆ is an          ┆        ┆ amazing!       │\n",
       "│                 ┆           ┆            ┆            ┆ extremely a…   ┆        ┆                │\n",
       "│ …               ┆ …         ┆ …          ┆ …          ┆ …              ┆ …      ┆ …              │\n",
       "│ 8 August 2006   ┆ tt0454848 ┆ ur9034662  ┆ true       ┆ Well, he       ┆ 2      ┆ don't bother   │\n",
       "│                 ┆           ┆            ┆            ┆ doesn't have   ┆        ┆                │\n",
       "│                 ┆           ┆            ┆            ┆ me foole…      ┆        ┆                │\n",
       "│ 24 November     ┆ tt0464154 ┆ ur19669890 ┆ true       ┆ Fish. Tits.    ┆ 3      ┆ A Portion Of   │\n",
       "│ 2010            ┆           ┆            ┆            ┆ Fish. Tits.    ┆        ┆ Fish And Tits, │\n",
       "│                 ┆           ┆            ┆            ┆ Fish. …        ┆        ┆ Pl…            │\n",
       "│ 16 June 2011    ┆ tt1605783 ┆ ur2444068  ┆ true       ┆ Gil (Owen      ┆ 8      ┆ A dream ride   │\n",
       "│                 ┆           ┆            ┆            ┆ Wilson) is a   ┆        ┆ in a classic   │\n",
       "│                 ┆           ┆            ┆            ┆ self-de…       ┆        ┆ Fren…          │\n",
       "│ 23 November     ┆ tt0454876 ┆ ur6983748  ┆ false      ┆ Astounding     ┆ 10     ┆ Movie of the   │\n",
       "│ 2012            ┆           ┆            ┆            ┆ movie          ┆        ┆ Year 2012      │\n",
       "│                 ┆           ┆            ┆            ┆ experience!! … ┆        ┆                │\n",
       "│ 12 August 2006  ┆ tt0415306 ┆ ur4839121  ┆ false      ┆ I was          ┆ 3      ┆ A mediocre     │\n",
       "│                 ┆           ┆            ┆            ┆ extremely      ┆        ┆ movie and a    │\n",
       "│                 ┆           ┆            ┆            ┆ disappointed   ┆        ┆ terribl…       │\n",
       "│                 ┆           ┆            ┆            ┆ i…             ┆        ┆                │\n",
       "└─────────────────┴───────────┴────────────┴────────────┴────────────────┴────────┴────────────────┘"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f0950df-80ea-4601-afee-51a5fa707359",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_processed = (\n",
    "    train_df.with_columns([\n",
    "        pl.col('review_text').map_elements(clean_text, return_dtype=pl.Utf8).alias(\"cleaned_text\")\n",
    "    ]).filter(\n",
    "            # Filter out empty strings and texts outside length bounds\n",
    "            (pl.col(\"cleaned_text\").str.len_chars() >= 10) &\n",
    "            (pl.col(\"cleaned_text\").str.len_chars() <= 10000) &\n",
    "            (pl.col(\"cleaned_text\") != \"\")\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "897acc38-84b1-46fd-b550-aedeeb2e2d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_processed = (\n",
    "    test_df.with_columns([\n",
    "        pl.col('review_text').map_elements(clean_text, return_dtype=pl.Utf8).alias(\"cleaned_text\")\n",
    "    ]).filter(\n",
    "            # Filter out empty strings and texts outside length bounds\n",
    "            (pl.col(\"cleaned_text\").str.len_chars() >= 10) &\n",
    "            (pl.col(\"cleaned_text\").str.len_chars() <= 10000) &\n",
    "            (pl.col(\"cleaned_text\") != \"\")\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92082d62-85f0-4954-8e2e-24282bea50ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 459109/459109 [04:12<00:00, 1819.44 examples/s]\n",
      "Map: 100%|██████████| 114780/114780 [01:03<00:00, 1814.93 examples/s]\n",
      "Saving the dataset (6/6 shards): 100%|██████████| 459109/459109 [00:08<00:00, 56998.88 examples/s]\n",
      "Saving the dataset (2/2 shards): 100%|██████████| 114780/114780 [00:01<00:00, 65123.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenize the datasets so we don't need to keep processing them!\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert Polars → Pandas → Hugging Face Dataset\n",
    "train_hf = Dataset.from_pandas(train_df_processed.to_pandas())\n",
    "test_hf = Dataset.from_pandas(test_df_processed.to_pandas())\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_batch(batch):\n",
    "    return hf_tokenizer(\n",
    "        batch[\"cleaned_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "train_hf = train_hf.map(tokenize_batch, batched=True)\n",
    "test_hf = test_hf.map(tokenize_batch, batched=True)\n",
    "\n",
    "# Save to disk\n",
    "train_hf.save_to_disk(\"imdb_train_tokenized\")\n",
    "test_hf.save_to_disk(\"imdb_test_tokenized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94017ddb-14ad-4bb9-8109-e4f80b013064",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "next time, we can load the dataset as such:\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "train_hf = load_from_disk(\"imdb_train_tokenized\")\n",
    "test_hf = load_from_disk(\"imdb_test_tokenized\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a09fea93-ea85-41ea-82e4-5a4fa3451a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b07a7e53-73cb-439f-9947-2271f6c4afe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29459e2f-a752-43f9-bf58-47d8b39743af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=30000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1b9b31d-335d-44e2-803c-802068662ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer.vocab_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
